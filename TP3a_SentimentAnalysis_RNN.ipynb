{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
    "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42",
    "id": "9nMRo1A3xy0b"
   },
   "source": [
    "# UJM - Master DSC/MLDM - Deep Learning - TP3a\n",
    "# Sentiment analysis with LSTM\n",
    "\n",
    "This session is based on this source: https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n",
    "\n",
    "**Associated data on claroline** : twitter.zip which contains 'Sentiment.csv'\n",
    "\n",
    "\n",
    "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243",
    "id": "H-PT5Fioxy0l"
   },
   "outputs": [],
   "source": [
    "# Notebook prepared with  Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
    "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
    "id": "L-bLdR5rxy0o"
   },
   "source": [
    "## First we process the data\n",
    "**Only keeping the necessary columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
    "id": "QWtiaDT_xy0p"
   },
   "outputs": [],
   "source": [
    "#We assume data to be in a directory data, change it with respect to your environment\n",
    "data = pd.read_csv('./data/Sentiment.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
    "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
    "id": "Xag2if3kxy0q"
   },
   "source": [
    "Next, the 'Neutral' sentiments are dropped as the goal proposed here is only to differentiate positive and negative tweets. After that, the tweets are filtered so that  only valid texts and words remain. The number of max features is defined as 2000 and we use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdSoWyk0xy0r",
    "outputId": "2ab29227-db11-4a17-97fd-0f5bb65e5a1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4472\n",
      "16986\n",
      "[   0    0    0    0    0    0    0    0    0    0    0  363  122    1\n",
      "  703    2   39   58  237   37  210    6  174 1761   12 1324 1409  743]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0  16 284 252   5 821 102 167\n",
      "  26 136   6   1 173  12   2 233 724  17]\n"
     ]
    }
   ],
   "source": [
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower()) #convert into lowercase letters\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) #remove symbols at beginning\n",
    "\n",
    "print(data[ data['sentiment'] == 'Positive'].size) #nb of positives\n",
    "print(data[ data['sentiment'] == 'Negative'].size) #nb of negatives\n",
    "\n",
    "for idx,row in data.iterrows():  #remove rt symbols\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "#We define the representation vector for each sequences. We use the tokenizer package  \n",
    "#Each sequence is represented by  a token, a word is a token\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X) \n",
    "\n",
    "#The last instruction pads each sequence with zeros at the beginning of the string representation\n",
    "#such that each string has 29 characters\n",
    "print(X[0])\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
    "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89",
    "id": "bDKy3PxYxy0t"
   },
   "source": [
    "Next, we design the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **dropout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note softmax is used as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "In1Ar_Fyxy0u",
    "outputId": "48c8e61a-5971-4a16-a7a3-d1fbf105f140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 28, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
    "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162",
    "id": "xMJHXw9Hxy0w"
   },
   "source": [
    "We now build the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRVB2NTvxy0x",
    "outputId": "81c7c69c-defc-49d7-be16-9c8af554d003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7188, 28) (7188, 2)\n",
      "\n",
      "\n",
      "(3541, 28) (3541, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print('\\n')\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7",
    "id": "xDU1hRADxy0y"
   },
   "source": [
    "Here we train the Network. We should run much more than 2 epochs, but to start, we fix it at 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cXx-wp-xy0z",
    "outputId": "5da2409c-d89c-4dfa-dd9b-3cc9033d012b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "225/225 - 33s - loss: 0.4416 - accuracy: 0.8173 - 33s/epoch - 146ms/step\n",
      "Epoch 2/2\n",
      "225/225 - 30s - loss: 0.3167 - accuracy: 0.8678 - 30s/epoch - 132ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4bd27d0090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 2, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
    "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12",
    "id": "yQCMpSPUxy0z"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
    "_uuid": "7872f6ea819a5d4d08394ba6db8436f9cb2cfe1c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9UXn9NTxy00",
    "outputId": "9533be11-7df4-423c-aba5-e528e3a27ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 1s - loss: 0.3427 - accuracy: 0.8540 - 1s/epoch - 22ms/step\n",
      "score: 0.34\n",
      "acc: 0.85\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
    "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c",
    "id": "OrvfEEVfxy01"
   },
   "source": [
    "Finally we measure the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. The \"bad\" results for positive tweets can be explained by the imbalanced nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "1add73e9-c6fb-7e4c-8715-ea92f519d2a6",
    "_uuid": "f80e9f3cf281adb3ab0357cbf6f886eb1dce3005",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgJHQ8hPxy01",
    "outputId": "84e87b96-99e1-4053-df94-e46549039c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 49.19093851132686 %\n",
      "neg_acc 96.55751469353484 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 0)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
    "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff",
    "id": "aSRAv97Qxy02"
   },
   "source": [
    "**We now predict some tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAwMyPiDxy02",
    "outputId": "42bf556a-c4a9-4152-c6df-083fe3821099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  206  633    6  150    5   55 1055   55   46    6  150]]\n",
      "1/1 - 0s - 23ms/epoch - 23ms/step\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c611b55c-92e4-4a33-8e82-1812bef6193d",
    "_uuid": "8b10995b0832ec98ba0c75832186fcb09b1a2d5f",
    "collapsed": true,
    "id": "0uOYBwfaxy02"
   },
   "source": [
    "Test your own tweets ! \n",
    "**Note**: this is a basic notebook. The model requires more epochs and special attention to class imbalance, one solution could be to use more data or to start from pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZK93a4ksxy03"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "colab": {
   "name": "TP3a_SentimentAnalysis_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
