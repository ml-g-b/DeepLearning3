{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
        "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42",
        "id": "9nMRo1A3xy0b"
      },
      "source": [
        "# UJM - Master DSC/MLDM - Deep Learning - TP3a\n",
        "# Sentiment analysis with LSTM\n",
        "\n",
        "This session is based on this source: https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n",
        "\n",
        "**Associated data on claroline** : twitter.zip which contains 'Sentiment.csv'\n",
        "\n",
        "\n",
        "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
        "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243",
        "id": "H-PT5Fioxy0l"
      },
      "outputs": [],
      "source": [
        "# Notebook prepared with  Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
        "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
        "id": "L-bLdR5rxy0o"
      },
      "source": [
        "## First we process the data\n",
        "**Only keeping the necessary columns.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
        "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
        "id": "QWtiaDT_xy0p"
      },
      "outputs": [],
      "source": [
        "#We assume data to be in a directory data, change it with respect to your environment\n",
        "data = pd.read_csv('./Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
        "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
        "id": "Xag2if3kxy0q"
      },
      "source": [
        "Next, the 'Neutral' sentiments are dropped as the goal proposed here is only to differentiate positive and negative tweets. After that, the tweets are filtered so that  only valid texts and words remain. The number of max features is defined as 2000 and we use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdSoWyk0xy0r",
        "outputId": "5e25f0e7-9ec4-416e-feaf-908fed9a1a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-1ad17580b112>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(lambda x: x.lower()) #convert into lowercase letters\n",
            "<ipython-input-11-1ad17580b112>:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  row[0] = row[0].replace('rt',' ')\n",
            "<ipython-input-11-1ad17580b112>:9: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
            "  row[0] = row[0].replace('rt',' ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3498\n",
            "12558\n",
            "[   0    0    0    0    0    0    0    0    0    0    3  268   98    2\n",
            "  563    1   17   27  292   32  136    6  150 1607   10  998 1061  673]\n",
            "[   0    0    0    0    0    0    0    0    0    3   16  206  227    7\n",
            "  757   78  115   25 1797   97    6    2  199   10    1  147  489   13]\n"
          ]
        }
      ],
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower()) #convert into lowercase letters\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) #remove symbols at beginning\n",
        "\n",
        "print(data[ data['sentiment'] == 'Positive'].size) #nb of positives\n",
        "print(data[ data['sentiment'] == 'Negative'].size) #nb of negatives\n",
        "\n",
        "for idx,row in data.iterrows():  #remove rt symbols\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "\n",
        "#We define the representation vector for each sequences. We use the tokenizer package\n",
        "#Each sequence is represented by  a token, a word is a token\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "#The last instruction pads each sequence with zeros at the beginning of the string representation\n",
        "#such that each string has 29 characters\n",
        "print(X[0])\n",
        "print(X[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
        "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89",
        "id": "bDKy3PxYxy0t"
      },
      "source": [
        "Next, we design the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **dropout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note softmax is used as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
        "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "In1Ar_Fyxy0u",
        "outputId": "799d4f6f-a6f1-4902-ebdc-bb5d340d1674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d (\u001b[38;5;33mSpatialDropout1D\u001b[0m) │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>) │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
        "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162",
        "id": "xMJHXw9Hxy0w"
      },
      "source": [
        "We now build the train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
        "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRVB2NTvxy0x",
        "outputId": "8306d375-4053-4751-cffc-e1f2a7507843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5378, 28) (5378, 2)\n",
            "\n",
            "\n",
            "(2650, 28) (2650, 2)\n"
          ]
        }
      ],
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print('\\n')\n",
        "print(X_test.shape,Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
        "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7",
        "id": "xDU1hRADxy0y"
      },
      "source": [
        "Here we train the Network. We should run much more than 2 epochs, but to start, we fix it at 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
        "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cXx-wp-xy0z",
        "outputId": "571d7fd5-0e22-4e93-8288-96680d725cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "169/169 - 15s - 87ms/step - accuracy: 0.7873 - loss: 0.5017\n",
            "Epoch 2/2\n",
            "169/169 - 12s - 72ms/step - accuracy: 0.8561 - loss: 0.3457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bf086c96bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 2, batch_size=batch_size, verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
        "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12",
        "id": "yQCMpSPUxy0z"
      },
      "source": [
        "Extracting a validation set, and measuring score and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
        "_uuid": "7872f6ea819a5d4d08394ba6db8436f9cb2cfe1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9UXn9NTxy00",
        "outputId": "ed5d3ac4-9ba4-4a04-ae6c-68545c4e2732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 - 3s - 83ms/step - accuracy: 0.8609 - loss: 0.3629\n",
            "score: 0.36\n",
            "acc: 0.86\n"
          ]
        }
      ],
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
        "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c",
        "id": "OrvfEEVfxy01"
      },
      "source": [
        "Finally we measure the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. The \"bad\" results for positive tweets can be explained by the imbalanced nature of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "_cell_guid": "1add73e9-c6fb-7e4c-8715-ea92f519d2a6",
        "_uuid": "f80e9f3cf281adb3ab0357cbf6f886eb1dce3005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgJHQ8hPxy01",
        "outputId": "eabec62f-6c75-46a6-ed4e-295ea0f3d423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_acc 44.47592067988669 %\n",
            "neg_acc 95.9023539668701 %\n"
          ]
        }
      ],
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "\n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 0)[0]\n",
        "\n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "\n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
        "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff",
        "id": "aSRAv97Qxy02"
      },
      "source": [
        "**We now predict some tweets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
        "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAwMyPiDxy02",
        "outputId": "d5730858-dbec-46db-f516-8fe351048491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 168\n",
            "  677   6  95   7  39 903  39  40   6  95]]\n",
            "1/1 - 0s - 23ms/step\n",
            "negative\n"
          ]
        }
      ],
      "source": [
        "twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c611b55c-92e4-4a33-8e82-1812bef6193d",
        "_uuid": "8b10995b0832ec98ba0c75832186fcb09b1a2d5f",
        "collapsed": true,
        "id": "0uOYBwfaxy02"
      },
      "source": [
        "Test your own tweets !\n",
        "**Note**: this is a basic notebook. The model requires more epochs and special attention to class imbalance, one solution could be to use more data or to start from pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZK93a4ksxy03",
        "outputId": "649f81c4-8d4b-4ad6-e4e9-324ef05831b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative -> 0.61238956\n",
            "positive -> 0.5688901\n",
            "negative -> 0.5228314\n",
            "positive -> 0.5688901\n",
            "negative -> 0.61238956\n",
            "positive -> 0.5688901\n"
          ]
        }
      ],
      "source": [
        "twts = ['This is dumb',\n",
        "        'I love pudding',\n",
        "        'But you are kind',\n",
        "        \"I don't think there are good and bad situations, if I had to sum up my life, I would say that this is before all some meetings\",\n",
        "        \"This situation is bad\",\n",
        "        \"I don't like country music\"]\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "for twt in twts:\n",
        "  twt = tokenizer.texts_to_sequences(twt)\n",
        "  #padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "  twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "  #print(twt)\n",
        "  sentiment = model.predict(twt,batch_size=1,verbose = 0)[0]\n",
        "  if(np.argmax(sentiment) == 0):\n",
        "      print(\"negative ->\", sentiment[0])\n",
        "  elif (np.argmax(sentiment) == 1):\n",
        "      print(\"positive ->\", sentiment[1])"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "colab": {
      "name": "TP3a_SentimentAnalysis_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}